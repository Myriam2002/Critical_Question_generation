{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirroring with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Components for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "load_dotenv()\n",
    "api_key = \"ge..\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the prompts for each LLM we have\n",
    "ALL_PLACEHOLDERS = [\n",
    "    \"[INTERVENTION_PLACEHOLDER]\",\n",
    "    \"[QUESTIONS_PLACEHOLDER]\",\n",
    "    \"[ML_RATING_FEEDBACK_PLACEHOLDER]\",\n",
    "    \"[LLM_FEEDBACK_PLACEHOLDER]\"\n",
    "]\n",
    "def build_config(agent_dict, row, extra_placeholders={}):\n",
    "    # Start with empty values for all placeholders\n",
    "    placeholders = {ph: \"\" for ph in ALL_PLACEHOLDERS}\n",
    "\n",
    "    # Always add intervention\n",
    "    placeholders[\"[INTERVENTION_PLACEHOLDER]\"] = row[\"intervention\"]\n",
    "\n",
    "    # Add whatever else is needed from the pipeline\n",
    "    for key, val in extra_placeholders.items():\n",
    "        if key in placeholders:\n",
    "            placeholders[key] = val\n",
    "\n",
    "    config = {\n",
    "        \"agent_type\": agent_dict[\"name\"],\n",
    "        \"prompt\": agent_dict[\"prompt\"],\n",
    "        \"description\": agent_dict[\"description\"],\n",
    "        \"intervention\": row[\"intervention\"],\n",
    "        \"placeholders\": placeholders\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ QUERYING THE MODEL\n",
    "\n",
    "def query_model(config):\n",
    "    openai = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    )\n",
    "    agent_type = config[\"agent_type\"]\n",
    "    prompt = config[\"prompt\"]\n",
    "    description = config[\"description\"]\n",
    "    intervention = config[\"intervention\"]\n",
    "    placeholders = config[\"placeholders\"]\n",
    "\n",
    "    # Always enforce the intervention placeholder to be accurate\n",
    "    placeholders[\"[INTERVENTION_PLACEHOLDER]\"] = intervention\n",
    "\n",
    "    # Replace placeholders in prompt\n",
    "    for placeholder, value in placeholders.items():\n",
    "        prompt = prompt.replace(placeholder, value or f\"<MISSING {placeholder}>\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": description},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    # Send request\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model= \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "\n",
    "    return response, prompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "api_key2 = os.getenv(\"OPENAI_API_KEY\")\n",
    "client2 = OpenAI(api_key=api_key2)\n",
    "def extract_CQ(response):\n",
    "    \n",
    "    # Send request\n",
    "    prompt = f\"Extract the 9 critical questions from the text below, returning only a Python list of the questions without numbering or formatting. For example, if the text contains '1. What is your name?' just include 'What is your name?' in the list.\\n\\nText: {response}\"\n",
    "\n",
    "    response = client2.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"o4-mini-2025-04-16\"\n",
    "    )\n",
    "    \n",
    "    # Get the string response\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    # Try to parse it as a Python list\n",
    "    try:\n",
    "        # Handle the case where the model returns a properly formatted list\n",
    "        import ast\n",
    "        return ast.literal_eval(response_text)\n",
    "    except (SyntaxError, ValueError):\n",
    "        # If parsing fails, do some cleanup and try again\n",
    "        # Remove common formatting issues\n",
    "        cleaned = response_text.strip()\n",
    "        if cleaned.startswith(\"```python\"):\n",
    "            cleaned = cleaned.split(\"```python\", 1)[1]\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = cleaned.split(\"```\", 1)[1]\n",
    "        if cleaned.endswith(\"```\"):\n",
    "            cleaned = cleaned.rsplit(\"```\", 1)[0]\n",
    "        \n",
    "        cleaned = cleaned.strip()\n",
    "        \n",
    "        try:\n",
    "            # Try parsing again\n",
    "            return ast.literal_eval(cleaned)\n",
    "        except (SyntaxError, ValueError):\n",
    "            # If still failing, split by newlines or other common separators\n",
    "            if \"[\" in cleaned and \"]\" in cleaned:\n",
    "                # Extract content between brackets\n",
    "                content = cleaned[cleaned.find(\"[\")+1:cleaned.rfind(\"]\")]\n",
    "                # Split by commas and clean up each item\n",
    "                items = [item.strip().strip(\"'\\\"\") for item in content.split(\",\")]\n",
    "                return [item for item in items if item]\n",
    "            else:\n",
    "                # Last resort - split by newlines\n",
    "                items = [line.strip().strip(\"'\\\"- \") for line in cleaned.split(\"\\n\")]\n",
    "                return [item for item in items if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ ML MODEL\n",
    "load_dir = \"D:\\\\My_working_area\\\\Masters\\\\Semester 2\\\\NLP804\\\\Project\\\\Critical_Question_generation\\\\models\"\n",
    "model = joblib.load(os.path.join(load_dir, \"logistic_model_with_features.joblib\"))\n",
    "vectorizer = joblib.load(os.path.join(load_dir, \"tfidf_vectorizer_with_bigrams.joblib\"))\n",
    "scaler = joblib.load(os.path.join(load_dir, \"structured_feature_scaler.joblib\"))\n",
    "\n",
    "#--------------Features and prediction of ML\n",
    "def predict_usefulness(intervention, question, features_dict):\n",
    "    text_input = intervention + \" \" + question\n",
    "    X_text_vec = vectorizer.transform([text_input])\n",
    "    X_struct = pd.DataFrame([features_dict])\n",
    "    X_struct_scaled = scaler.transform(X_struct)\n",
    "    X_combined = hstack([X_text_vec, X_struct_scaled])\n",
    "\n",
    "    proba = model.predict_proba(X_combined)[0]\n",
    "    return {\n",
    "        \"Invalid\": round(proba[0], 4),\n",
    "        \"Unhelpful\": round(proba[1], 4),\n",
    "        \"Useful\": round(proba[2], 4)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_features(intervention, question):\n",
    "    \"\"\"\n",
    "    Automatically compute the structured features needed for prediction.\n",
    "    \"\"\"\n",
    "    # Word count & char count\n",
    "    question_word_count = len(question.split())\n",
    "    question_char_count = len(question)\n",
    "\n",
    "    # Word overlap\n",
    "    intervention_tokens = set(intervention.lower().split())\n",
    "    question_tokens = set(question.lower().split())\n",
    "    overlap = len(intervention_tokens & question_tokens)\n",
    "    word_overlap = overlap / max(len(question_tokens), 1)\n",
    "\n",
    "    # BM25 similarity approximation using bag-of-words cosine similarity\n",
    "    vectorizer = CountVectorizer().fit([intervention, question])\n",
    "    vecs = vectorizer.transform([intervention, question]).toarray()\n",
    "    cosine_similarity = np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1]) + 1e-10)\n",
    "\n",
    "    # Max similarity placeholder\n",
    "    max_similarity = cosine_similarity\n",
    "    return {\n",
    "        \"question_word_count\": question_word_count,\n",
    "        \"question_char_count\": question_char_count,\n",
    "        \"bm25_similarity\": round(cosine_similarity, 4),\n",
    "        \"word_overlap\": round(word_overlap, 4),\n",
    "        \"max_similarity\": round(max_similarity, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker(intervention, list_of_cq):\n",
    "    # Make sure list_of_cq is a list\n",
    "    if isinstance(list_of_cq, str):\n",
    "        # If it's a string with newlines, split by newlines\n",
    "        list_of_cq = list_of_cq.split('\\n')\n",
    "        # Clean up any empty strings or whitespace-only strings\n",
    "        list_of_cq = [q.strip() for q in list_of_cq if q.strip()]\n",
    "    \n",
    "    rankings = []\n",
    "    for i in list_of_cq:\n",
    "        features = compute_features(intervention, i)\n",
    "        x = predict_usefulness(intervention, i, features)\n",
    "        rankings.append((i, x[\"Useful\"]))\n",
    "    \n",
    "    # Sort the rankings list based on the second element (usefulness score)\n",
    "    # in descending order (from highest to lowest)\n",
    "    sorted_rankings = sorted(rankings, key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Extract just the questions in ranked order\n",
    "    ranked_questions = [item[0] for item in sorted_rankings]\n",
    "    \n",
    "    # Return both the full rankings (with scores) and just the ranked questions\n",
    "    return sorted_rankings, ranked_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QC_LLM_generator: {'name': 'Critical Question Generator', 'description': 'This agent creates a list of 9 Critical Questions for a given intervention. The questions must be useful by challenging the assumptions made in the text while avoiding peripheral details, obvious answers, and new jargon. The model should think deeply about the intervention and work through its reasoning step-by-step before generating the final questions.', 'prompt': 'You are a teacher in a critical thinking class. Your goal is to help students learn to critically evaluate argumentative texts. To do this, you need to generate critical questions that challenge the validity of the arguments presented. A question is considered USEFUL if it makes the reader reflect on the text in a way that could potentially diminish its perceived validity. Avoid questions that are common sense, reading-comprehension, too general, or that introduce new concepts not present in the text.\\n\\nGuidelines:\\n1. USEFUL QUESTION:\\n   - Challenges the textâ€™s argument in a meaningful way.\\n   - Prompts critical reflection that can weaken the argumentâ€™s validity if answered.\\n   - Focuses on details already present in the text without introducing external ideas.\\n\\n2. UNHELPFUL QUESTION:\\n   - Although related to the text, it asks about aspects that are either common sense, well-known facts, or too complicated.\\n\\n3. INVALID QUESTION:\\n   - Unrelated to the text or introduces new concepts.\\n   - Uses vague language or fails to challenge the argumentâ€™s core reasoning.\\n\\nFew-Shot Examples:\\n\\nExample 1\\nTEXT:\\n\"We have indeed required those products to have very large warnings on them, telling people about their salt and fat content. And we don\\'t allow them to say things which are misleading. We don\\'t allow fatty products to say they\\'re healthy... There are two figures on this letter which really stand out: 0% per annum new balance transfers, 0% per annum new money transfers. Now, underneath that, in small print, it becomes absolutely clear that you\\'ll be paying all sorts of charges. And then very, very quickly you\\'ll be lured into paying a very large charge.\"\\n\\nUseful Questions:\\n- What is the basis for comparing the credit card company\\'s advertisement to food products, and is this comparison valid?\\n- Is there evidence that prevents misleading descriptions of fatty products, and does the text support this claim?\\n- Does the credit card companyâ€™s offer of 0% interest truly reflect the cost of borrowing, given the hidden fees?\\n\\nUnhelpful Question:\\n- How does the speaker know that the advertisement is irresponsible? (This is already stated in the text.)\\n\\nInvalid Question:\\n- Are there any assumptions made by the speaker that are not explicitly stated? (Too generic.)\\n\\nNow, using the guidelines and examples above, generate three USEFUL critical questions for the following text:\\n\\nTEXT:\\n\"[INTERVENTION_PLACEHOLDER]\"\\n\\nProvide one question per line. Do not include any special characters or numbering except for the question mark.'}\n",
      "QC_LLM_evaluator: {'name': 'Critical Question Evaluator', 'description': 'This agent reviews the list of Critical Questions and provides detailed, step-by-step feedback. The evaluation should consider whether each question challenges the assumptions in the intervention and stays within scope. Unhelpful questions might focus on peripheral details, offer obvious answers, or introduce new jargon. Invalid questions stray from the intervention. The model should reason deeply through each question before finalizing its evaluation.', 'prompt': 'In this assignment, you should imagine that you are a teacher in a class on critical thinking. During this class, your goal is to get students to critically analyze the argumentative texts presented to them and provide them with the tools to evaluate the validity of the arguments in the text. To achieve this goal, students will be given a paragraph with argumentative content, and you should pose questions to them about this paragraph. You should read the paragraph, look at the questions we suggest, and then assess which of these questions might be useful to the students and which ones might not. For each of the questions, you will have to decide if it is useful, unhelpful, or not even valid for the arguments of that paragraph. Here we define these categories:\\n\\nGuidelines:\\n\\n1. USEFUL QUESTION:\\n   - One should not take the arguments in the text as valid without having reflected on this question.\\n\\n2. UNHELPFUL QUESTION:\\n   - The question makes sense and is related to the text but is unlikely to be useful for critical analysis.\\n   - The answer to the question might be common sense or a well-known fact that does not generate controversy.\\n   - The question might be too complex to be practical or is already answered in the text.\\n\\n3. INVALID QUESTION:\\n   - The answer to this question cannot serve to invalidate or diminish the acceptability of the arguments in the text.\\n   - This may happen if:\\n     a. The question is unrelated to the text.\\n     b. The question introduces new concepts that were not in the text.\\n     c. The question does not challenge any argument in the text.\\n     d. The question is too vague and could be applied to any text.\\n     e. The question is a reading-comprehension question rather than a critical-thinking question.\\n\\nBelow are examples of texts along with categorized questions:\\n\\nExample 1:\\n\\nTEXT:\\nMT: \"We have indeed required those products to have very large warnings on them, telling people about their salt and fat content. And we don\\'t allow them to say things which are misleading. We don\\'t allow fatty products to say they\\'re healthy. I\\'ve got in front of me a letter from a credit card company. There are two figures on this letter which really stand out: 0% per annum new balance transfers, 0% per annum new money transfers. Now, underneath that, in small print, it becomes absolutely clear that you\\'ll be paying all sorts of charges. And then very, very quickly you\\'ll be lured into paying a very large charge. Do you think it\\'s responsible to encourage people to mis-read something like that? I\\'ve got all the information there in front of me.\"\\n\\nUSEFUL QUESTIONS:\\n- What is the basis for comparing the credit card company\\'s advertisement to food products, and is this comparison valid?\\n- Is it actually the case that fatty products that have potential bad consequences are not allowed to be described in misleading ways? Is there evidence for this claim?\\n- Are there other factors in this particular case that could have interfered with the event of \\'people being lured into paying a very large charge\\'?\\n- Does the credit card company\\'s offer of 0% interest rates for new balances and transfers accurately reflect the true cost of borrowing? What hidden fees or charges might consumers face?\\n- How does the mentioned credit card offer differ between its initial presentation and fine print details?\\n- What specific charges does the credit card company advertise as being 0% per annum, and how do these charges compare to the charges that will actually be paid?\\n\\nUNHELPFUL QUESTIONS:\\n- How does the speaker know that the credit card company\\'s advertisement is irresponsible, and what evidence does the speaker have to support this claim? â†’ it says it in the text.\\n- How might the credit card company\\'s advertisement be improved to make it more responsible and less misleading, and what changes would need to be made to achieve this? â†’ answering this is very complicated and not to the point.\\n- How strong is the generalization that if all sorts of charges are written in small print then people will be lured into paying a very large charge? â†’ to some extent, this is common sense.\\n\\nINVALID QUESTIONS:\\n- Are there any assumptions made by the speaker that are not explicitly stated but are implied by their argument? â†’ this question could be raised for any text, itâ€™s not specific.\\n- Are these measures effective in preventing misleading claims by manufacturers? â†’ no measures are being proposed in this text.\\n- Is the speaker\\'s argument based on a logical chain of reasoning, or are there any gaping holes or flaws in the argument? â†’ this question could be raised for any text, itâ€™s not specific.\\n\\nExample 2:\\n\\nTEXT:\\nJW: \"Well, debt is morally neutral in the sense that it can be good to be in debt. Sometimes it\\'s a good idea, and sometimes it\\'s a bad idea. And so I think there\\'s been broad agreement about that in the discussion so far. The question is why is there too much debt? Why are people taking on too much debt? Now, a lot of people come up with rather glib moral answers: somehow bankers want them to get into excessive debt, which is a very peculiar idea, when you think that it\\'s the bankers who aren\\'t going to get repaid. So it\\'s a very odd idea. There\\'s too much debt for the same reason that there are too many unmarried mothers, and for the same reason that you get an overproduction of sheep.\"\\n\\nUSEFUL QUESTIONS:\\n- If debt is considered \\'morally neutral\\', then how do we determine whether someone\\'s level of debt is acceptable or not? Are there specific circumstances under which debt becomes immoral or irresponsible?\\n- What is being defined as \\'too much debt\\'?\\n- How does the concept of \\'excessive debt\\' differ from simply having debt? In other words, at what point does debt become problematic?\\n- How does the comparison to unmarried mothers and overproduction of sheep relate to the issue of debt?\\n\\nUNHELPFUL QUESTIONS:\\n- What evidence supports that it is generally accepted that debt is morally neutral in the sense that it can be good to be in debt? â†’ this question is too complicated to answer.\\n- What are the potential consequences of labeling debt as \\'good\\' or \\'bad\\'? â†’ this question is too complicated to answer.\\n\\nINVALID QUESTIONS:\\n- How does JW suggest we approach understanding why there is too much debt in society? Does he offer any alternative perspectives beyond those already discussed in the conversation? â†’ this is a reading-comprehension question.\\n- Why might banks encourage people to take on more debt than they can afford? Are they acting in their own self-interest, or are there broader societal forces at play here? â†’ the text does not say that banks encourage people to take more debt.\\n\\nYour task is to classify each suggested question as useful, unhelpful, or invalid, and provide a brief justification for your classification.\\n\\n---------------------------\\n\\nEvaluation Task:\\n\\nNow, based on the guidelines above, you will be given an intervention (a paragraph from an argumentative text) and a question that attempts to challenge it. Your task is to classify the question into one of the three categories: USEFUL, UNHELPFUL, or INVALID. Provide the label in the following format:\\n\\nIntervention:\\n[INTERVENTION_PLACEHOLDER]\\n\\nQuestion:\\n[QUESTIONS_PLACEHOLDER]\\n\\nANSWER:'}\n",
      "QC_LLM_improver_ml: {'name': 'Critical Question Improver (ML Rated)', 'description': 'Improves critical questions using ML rating feedback. It analyzes the questions, applies ML feedback, and produces a ranked list of improved questions.', 'prompt': \"You are provided with the following:\\n- An intervention: [INTERVENTION_PLACEHOLDER]\\n- A list of critical questions with their usefulness score from 0-1: [ML_RATING_FEEDBACK_PLACEHOLDER]\\n\\n\\n1. Carefully analyze each question's strengths and weaknesses based on the intervention and ML feedback.\\n2. Revise each question to be clearer, more specific, and better at challenging key assumptions, exploring risks, or proposing alternative perspectives.\\n3. Avoid vague, obvious, or out-of-scope questions.\\n4. Integrate the ML feedback carefully for improvements.\\n\\nFinally:\\n- Return a list of improved questions, ranked from strongest to weakest based on quality and depth.\\n- Make sure the questions are open-ended and meaningful for critical analysis.\"}\n",
      "QC_LLM_improver_llm: {'name': 'Critical Question Improver (LLM Feedback Only)', 'description': 'Improves critical questions using evaluator (LLM) feedback. It analyzes the questions, applies LLM feedback, and produces a ranked list of improved questions.', 'prompt': \"You are provided with the following:\\n- An intervention: [INTERVENTION_PLACEHOLDER]\\n- A list of critical questions: [QUESTIONS_PLACEHOLDER]\\n\\n  Also use:\\n- Evaluator (LLM) feedback: [LLM_FEEDBACK_PLACEHOLDER]\\n\\nYour task is to:\\n1. Carefully analyze each question's strengths and weaknesses based on the intervention and LLM feedback.\\n2. Revise each question to be clearer, more specific, and better at challenging key assumptions, exploring risks, or proposing alternative perspectives.\\n3. Avoid vague, obvious, or out-of-scope questions.\\n4. Integrate the evaluator's feedback carefully for improvements.\\n\\nFinally:\\n- Return a list of improved questions, ranked from strongest to weakest based on quality and depth.\\n- Make sure the questions are open-ended and meaningful for critical analysis.\"}\n",
      "QC_LLM_improver_both: {'name': 'Critical Question Improver (ML Rated and LLM Feedback)', 'description': 'Improves critical questions using both ML and LLM evaluator feedback. It analyzes the original questions, applies all feedback available, and produces a ranked list of improved questions.', 'prompt': \"You are provided with the following:\\n- An intervention: [INTERVENTION_PLACEHOLDER]\\n- A list of Critical question with their usefulness score from 0-1:[ML_RATING_FEEDBACK_PLACEHOLDER]\\n- Evaluator (LLM) feedback: [LLM_FEEDBACK_PLACEHOLDER]\\n\\nYour task is to:\\n1. Carefully analyze each question's strengths and weaknesses based on the intervention and all available feedback.\\n2. Revise each question to be clearer, more specific, and better at challenging key assumptions, exploring risks, or proposing alternative perspectives.\\n3. Avoid vague, obvious, or out-of-scope questions.\\n4. Integrate both ML and LLM feedback carefully for improvements.\\n\\nFinally:\\n- Return a list of improved questions, ranked from strongest to weakest based on quality and depth.\\n- Make sure the questions are open-ended and meaningful for critical analysis.\"}\n",
      "\n",
      "Generator Details:\n",
      "Name: Critical Question Generator\n",
      "Description: This agent creates a list of 9 Critical Questions for a given intervention. The questions must be useful by challenging the assumptions made in the text while avoiding peripheral details, obvious answers, and new jargon. The model should think deeply about the intervention and work through its reasoning step-by-step before generating the final questions.\n",
      "Prompt: You are a teacher in a critical thinking class. Your goal is to help students learn to critically evaluate argumentative texts. To do this, you need to generate critical questions that challenge the validity of the arguments presented. A question is considered USEFUL if it makes the reader reflect on the text in a way that could potentially diminish its perceived validity. Avoid questions that are common sense, reading-comprehension, too general, or that introduce new concepts not present in the text.\n",
      "\n",
      "Guidelines:\n",
      "1. USEFUL QUESTION:\n",
      "   - Challenges the textâ€™s argument in a meaningful way.\n",
      "   - Prompts critical reflection that can weaken the argumentâ€™s validity if answered.\n",
      "   - Focuses on details already present in the text without introducing external ideas.\n",
      "\n",
      "2. UNHELPFUL QUESTION:\n",
      "   - Although related to the text, it asks about aspects that are either common sense, well-known facts, or too complicated.\n",
      "\n",
      "3. INVALID QUESTION:\n",
      "   - Unrelated to the text or introduces new concepts.\n",
      "   - Uses vague language or fails to challenge the argumentâ€™s core reasoning.\n",
      "\n",
      "Few-Shot Examples:\n",
      "\n",
      "Example 1\n",
      "TEXT:\n",
      "\"We have indeed required those products to have very large warnings on them, telling people about their salt and fat content. And we don't allow them to say things which are misleading. We don't allow fatty products to say they're healthy... There are two figures on this letter which really stand out: 0% per annum new balance transfers, 0% per annum new money transfers. Now, underneath that, in small print, it becomes absolutely clear that you'll be paying all sorts of charges. And then very, very quickly you'll be lured into paying a very large charge.\"\n",
      "\n",
      "Useful Questions:\n",
      "- What is the basis for comparing the credit card company's advertisement to food products, and is this comparison valid?\n",
      "- Is there evidence that prevents misleading descriptions of fatty products, and does the text support this claim?\n",
      "- Does the credit card companyâ€™s offer of 0% interest truly reflect the cost of borrowing, given the hidden fees?\n",
      "\n",
      "Unhelpful Question:\n",
      "- How does the speaker know that the advertisement is irresponsible? (This is already stated in the text.)\n",
      "\n",
      "Invalid Question:\n",
      "- Are there any assumptions made by the speaker that are not explicitly stated? (Too generic.)\n",
      "\n",
      "Now, using the guidelines and examples above, generate three USEFUL critical questions for the following text:\n",
      "\n",
      "TEXT:\n",
      "\"[INTERVENTION_PLACEHOLDER]\"\n",
      "\n",
      "Provide one question per line. Do not include any special characters or numbering except for the question mark.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(\"prompt_agents.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "# Extract each agent's details into individual variables\n",
    "QC_LLM_generator = data[\"agents\"][0]\n",
    "QC_LLM_evaluator = data[\"agents\"][1]\n",
    "QC_LLM_improver_ml = data[\"agents\"][2]\n",
    "QC_LLM_improver_llm = data[\"agents\"][3]\n",
    "QC_LLM_improver_both = data[\"agents\"][4]\n",
    "\n",
    "# Now, each variable holds a dictionary with keys: \"name\", \"description\", and \"prompt\".\n",
    "# For example, you can print the details of each agent as follows:\n",
    "print(\"QC_LLM_generator:\", QC_LLM_generator)\n",
    "print(\"QC_LLM_evaluator:\", QC_LLM_evaluator)\n",
    "print(\"QC_LLM_improver_ml:\", QC_LLM_improver_ml)\n",
    "print(\"QC_LLM_improver_llm:\", QC_LLM_improver_llm)\n",
    "print(\"QC_LLM_improver_both:\", QC_LLM_improver_both)\n",
    "\n",
    "# Example usage: print generator details individually\n",
    "print(\"\\nGenerator Details:\")\n",
    "print(\"Name:\", QC_LLM_generator[\"name\"])\n",
    "print(\"Description:\", QC_LLM_generator[\"description\"])\n",
    "print(\"Prompt:\", QC_LLM_generator[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intervention_id</th>\n",
       "      <th>intervention</th>\n",
       "      <th>dataset</th>\n",
       "      <th>schemes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MT_45</td>\n",
       "      <td>MT: \"Claire’s absolutely right about that\\nBut...</td>\n",
       "      <td>moral_maze_schemes</td>\n",
       "      <td>CauseToEffect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd38_220_2</td>\n",
       "      <td>cd38: \"The return flight is 6 ½ hours (plus co...</td>\n",
       "      <td>rrd</td>\n",
       "      <td>ERExample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRUMP_183</td>\n",
       "      <td>TRUMP: \"I do want to say that I was just endor...</td>\n",
       "      <td>US2016</td>\n",
       "      <td>PositionToKnow, Values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AngelComa__638</td>\n",
       "      <td>AngelComa: \"Funny, I thought he did good\\nThey...</td>\n",
       "      <td>us2016reddit</td>\n",
       "      <td>Ad hominem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRUMP_114_2</td>\n",
       "      <td>TRUMP: \"our country 's a mess\\nit 's one thing...</td>\n",
       "      <td>US2016</td>\n",
       "      <td>VerbalClassification, Example, CircumstantialA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  intervention_id                                       intervention  \\\n",
       "0           MT_45  MT: \"Claire’s absolutely right about that\\nBut...   \n",
       "1      cd38_220_2  cd38: \"The return flight is 6 ½ hours (plus co...   \n",
       "2       TRUMP_183  TRUMP: \"I do want to say that I was just endor...   \n",
       "3  AngelComa__638  AngelComa: \"Funny, I thought he did good\\nThey...   \n",
       "4     TRUMP_114_2  TRUMP: \"our country 's a mess\\nit 's one thing...   \n",
       "\n",
       "              dataset                                            schemes  \n",
       "0  moral_maze_schemes                                      CauseToEffect  \n",
       "1                 rrd                                          ERExample  \n",
       "2              US2016                             PositionToKnow, Values  \n",
       "3        us2016reddit                                         Ad hominem  \n",
       "4              US2016  VerbalClassification, Example, CircumstantialA...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:\\\\My_working_area\\\\Masters\\\\Semester 2\\\\NLP804\\\\Project\\\\Critical_Question_generation\\\\data_splits\\\\testing_dataset.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare processed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Process each intervention (ignore individual questions)\n",
    "for intervention_id, content in data.items():\n",
    "    intervention_text = content.get(\"intervention\", \"\")\n",
    "    dataset = content.get(\"dataset\", \"\")\n",
    "    scheme_types = set(content.get(\"schemes\", []))\n",
    "\n",
    "    preprocessed_data.append({\n",
    "        \"intervention_id\": intervention_id,\n",
    "        \"intervention\": intervention_text,\n",
    "        \"dataset\": dataset,\n",
    "        \"schemes\": \", \".join(scheme_types)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and drop duplicates by intervention_id\n",
    "df = pd.DataFrame(preprocessed_data).drop_duplicates(subset=\"intervention_id\").reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import numpy\n",
    "from collections import Counter\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from prompts_eval import *\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "logger = logging.getLogger(__name__)\n",
    "from evaluation import *\n",
    "def evaluate_results(target_file, result_file_name):\n",
    "    golden_path = \"D:\\\\My_working_area\\\\Masters\\\\Semester 2\\\\NLP804\\\\Project\\\\Critical_Question_generation\\\\data_splits\\\\testing_dataset.json\"\n",
    "    print(\"Starting\", target_file)\n",
    "    result  = eval_func(threshold = 0.6, golden_path=  golden_path, submission_path = target_file)\n",
    "    with open(result_file_name, 'w') as o:\n",
    "        json.dump(result, o, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Mirroring Agents and Evaluate with ML model only\n",
    "<img src=\"images\\approach_llm_eval_only.png\" alt=\"image\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION======= 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION======= 1\n",
      "QUESTION======= 2\n",
      "✅ Results saved\n",
      "Starting critical_questions/Approach_2_LLM_evaluator_llama405.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the labels: Counter({'Useful': 9})\n",
      "Distribution of the intervention punctuation: Counter({1.0: 3})\n",
      "Overall punctuation 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "results_to_analyze = []\n",
    "\n",
    "target_file = \"critical_questions/Approach_2_LLM_evaluator_llama405.json\"\n",
    "result_file_name_evaluation = \"evaluations/Evaluation_approach_2_llm_evaluator_llama405.json\"\n",
    "for i, row in df.iterrows(): \n",
    "    print(\"QUESTION=======\",i)\n",
    "    # Step 1: Generate questions\n",
    "    gen_config = build_config(QC_LLM_generator, row)\n",
    "    generated_questions,prompt_generator = query_model(gen_config)\n",
    "    \n",
    "    # Step 2: Evaluate those questions using LLM/ML\n",
    "    # Example use\n",
    "    intervention = row[\"intervention\"]\n",
    "    question = generated_questions\n",
    "    list_of_cq = extract_CQ(question)\n",
    "    formatted_list_cq =  \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(list_of_cq)])\n",
    "    # computed_features = compute_features(intervention, question)\n",
    "    # ML_result = predict_usefulness(intervention, question, computed_features)\n",
    "\n",
    "    eval_config = build_config(QC_LLM_evaluator, row, {\n",
    "        \"[QUESTIONS_PLACEHOLDER]\": formatted_list_cq\n",
    "    })\n",
    "    eval_feedback, prompt_evaluator = query_model(eval_config)\n",
    "\n",
    "    # Step 3: Improve questions with LLM feedback\n",
    "    improve_config = build_config(QC_LLM_improver_llm, row, {\n",
    "        \"[QUESTIONS_PLACEHOLDER]\": formatted_list_cq,\n",
    "        \"[LLM_FEEDBACK_PLACEHOLDER]\": eval_feedback\n",
    "    })\n",
    "    improved_questions, prompt_improver = query_model(improve_config)\n",
    "\n",
    "    #Step 4: Extract the solution from the problem\n",
    "    list_answer = extract_CQ(improved_questions)\n",
    "    #Step 5: Rerank the Questions\n",
    "    list_ordered_score, list_ordered_cq = reranker(intervention,list_answer)\n",
    "    # Step 6: Save all results along with the intervention\n",
    "    concatenated_answer = \"\\n\".join([f\"- {question}\" for question in list_ordered_cq[:3]])\n",
    "    \n",
    "    results_to_analyze.append({\n",
    "        \"intervention_id\": row[\"intervention_id\"],\n",
    "        \"intervention\": row[\"intervention\"],\n",
    "        \"prompt_generator\": prompt_generator,\n",
    "        \"prompt_evaluator\": prompt_evaluator ,\n",
    "        \"prompt_improver\": prompt_improver,\n",
    "        \"generated_questions\": generated_questions,\n",
    "        \"LLM_feedback\":eval_feedback,\n",
    "        \"improved_questions\": improved_questions,\n",
    "        \"List_of_questions\": list_answer,\n",
    "        \"list_of_questions_order_score\": list_ordered_score,\n",
    "        \"list_of_questions_order\":list_ordered_cq,\n",
    "        \"concatenated_answer\": concatenated_answer\n",
    "    })\n",
    "\n",
    "    results[row[\"intervention_id\"]] = {\n",
    "    \"intervention_id\": row[\"intervention_id\"],\n",
    "    \"intervention\": row[\"intervention\"],\n",
    "    \"dataset\": row[\"dataset\"],\n",
    "    \"schemes\": [scheme.strip() for scheme in row[\"schemes\"].split(\",\")],\n",
    "    \"cqs\": [\n",
    "        {\"id\": 0, \"cq\": list_ordered_cq[0]},\n",
    "        {\"id\": 1, \"cq\": list_ordered_cq[1]},\n",
    "        {\"id\": 2, \"cq\": list_ordered_cq[2]},\n",
    "    ],\n",
    "    \"full_response\": concatenated_answer\n",
    "    }\n",
    "with open(target_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "with open(\"double_check/approach_2_LLM_evaluator_llama405.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_to_analyze, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Results saved\")\n",
    "evaluate_results(target_file, result_file_name_evaluation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
